{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import sys  \n",
    "sys.path.insert(0, '../src')\n",
    "from dnns3 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_previous_store_lower(i, dfs):\n",
    "    \"\"\" Helper method used in prep_dataFrame.\n",
    "        We assume df has a column called 'v0-l' and that if the \n",
    "        index is j then row j contains the data for the j^th\n",
    "        instruction. We assume that df contains only the stores.\n",
    "        \n",
    "        We grab the lower 32 (least significant) bits of the\n",
    "        first store which occurs prior to the i^th instruction\n",
    "        and return this value.\n",
    "    \"\"\"\n",
    "    idx = dfs[dfs.index < i].index.max()\n",
    "    if (pd.isnull(idx)):\n",
    "        return np.nan\n",
    "    return dfs.loc[idx, 'v0-l']\n",
    "\n",
    "def get_previous_store_upper(i, dfs):\n",
    "    idx = dfs[dfs.index < i].index.max()\n",
    "    if (pd.isnull(idx)):\n",
    "        return np.nan\n",
    "    return dfs.loc[idx, 'v0-u']\n",
    "\n",
    "def get_previous_store_lower2(i, dfs):\n",
    "    \"\"\" Helper method used in prep_dataFrame.\n",
    "        We assume df has a column called 'v0-l' and that if the \n",
    "        index is j then row j contains the data for the j^th\n",
    "        instruction. We assume that df contains only the stores.\n",
    "        \n",
    "        We grab the lower 32 (least significant) bits of the\n",
    "        first store which occurs prior to the i^th instruction\n",
    "        and return this value.\n",
    "    \"\"\"\n",
    "    idx = dfs[dfs.index < i].index.max()\n",
    "    idx = dfs[dfs.index < idx].index.max()\n",
    "    if (pd.isnull(idx)):\n",
    "        return np.nan\n",
    "    return dfs.loc[idx, 'v0-l']\n",
    "\n",
    "def get_previous_store_upper2(i, dfs):\n",
    "    idx = dfs[dfs.index < i].index.max()\n",
    "    idx = dfs[dfs.index < idx].index.max()\n",
    "    if (pd.isnull(idx)):\n",
    "        return np.nan\n",
    "    return dfs.loc[idx, 'v0-u']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepDataFrame(filename, k=4, read_stores=True):\n",
    "\t\"\"\" Read in the loads and stores from a trace and \n",
    "\t\"\"\"\n",
    "\n",
    "\t# Read in the data and get it ready for computation\n",
    "\tdf = pd.read_csv(filename)\n",
    "\tdf = df.replace('NAN', np.nan)\n",
    "\tdf['pc'] = df['pc'].astype('int64')\n",
    "\tdf['effective_address'] = df['effective_address'].astype('int64')\n",
    "\tdf['val0'] = df['val0'].astype('int64')\n",
    "    \n",
    "    # Bits: uu mu ml ll\n",
    "    # uu := most significant group of 16 bits\n",
    "    # mu := 2nd most significant group of 16 bits\n",
    "    # ml := 3rd most significant group of 16 bits\n",
    "    # ll := least significant group of 16 bits\n",
    "\n",
    "\t# Build a dataframe with the 64bit values split into four 16 bit values\n",
    "\tdf1 = pd.DataFrame(index=df[df['type'] == 'l'].index) # only the loads\n",
    "\tdf1['pc-ll'] = df['pc'] % math.pow(2, 15) # lower 16 bits\n",
    "    df1['pc-ml'] = (df['pc'] - df1['pc-ll']) % math.pow(2, 31) \n",
    "\tdf1['ea-l'] = df['effective_address'] % math.pow(2, 15) # lower 16 bits\n",
    "\tdf1['ea-u'] = df['effective_address'] - df1['ea-l'] # upper 48 bits\n",
    "    \n",
    "    \n",
    "    \n",
    "    #TODO\n",
    "\tdf1['v0-l'] = df['val0'] % math.pow(2, 31)\n",
    "\tdf1['v0-u'] = df['val0'] - df1['v0-l']\n",
    "\n",
    "\t# Scale all values in df1 into the interval [0,1]\n",
    "\tdf1['pc-l'] = df1['pc-l']/df1['pc-l'].max()\n",
    "\tdf1['ea-l'] = df1['ea-l']/df1['ea-l'].max()\n",
    "\tdf1['ea-u'] = df1['ea-u']/df1['ea-u'].max()\n",
    "\tdf1['v0-l'] = df1['v0-l']/df1['v0-l'].max()\n",
    "\tdf1['v0-u'] = df1['v0-u']/df1['v0-u'].max()\n",
    "\n",
    "\t# Add columns for the previous k loads\n",
    "\tfor i in range(1, k+1):\n",
    "\t    lname = 'v0l-' + str(i)\n",
    "\t    df1[lname] = df1['v0-l'].shift(i)\n",
    "\t    uname = 'v0u-' + str(i)\n",
    "\t    df1[uname] = df1['v0-u'].shift(i)\n",
    "\n",
    "\t# Add columns for the first 32 and last 32 bits of the previous 2 store values\n",
    "\t# Create the file name where the store columns must be read/written from/to.\n",
    "\tstore_file = filename[:-4] + 'store_cols.csv'\n",
    "\tif (read_stores):\n",
    "\t\t# The columns for stores have already been computed for this file. \n",
    "\t\t# Read the file and put the columns in df1\n",
    "\t\tstore_columns = pd.read_csv(store_file, index_col=0)\n",
    "\t\tdf1['s-1l'] = store_columns['s-1l'].astype('float64')\n",
    "\t\tdf1['s-1u'] = store_columns['s-1u'].astype('float64')\n",
    "\t\tdf1['s-2l'] = store_columns['s-2l'].astype('float64')\n",
    "\t\tdf1['s-2u'] = store_columns['s-2u'].astype('float64')\n",
    "\telse:\n",
    "\t\t# The columns for the stroes have not been previously computed. Compute\n",
    "\t\t# these columns and write them to a file for later use\n",
    "\t\tdfs = pd.DataFrame(index=df[df['type'] == 's'].index)\n",
    "\t\tdfs['v0-l'] = (df['val0'] % math.pow(2, 31)).astype('float64')\n",
    "\t\tdfs['v0-u'] = (df['val0'] - dfs['v0-l']).astype('float64')\n",
    "\t\tdfs['v0-l'] = dfs['v0-l']/dfs['v0-l'].max()\n",
    "\t\tdfs['v0-u'] = dfs['v0-u']/dfs['v0-u'].max()\n",
    "\t\tdf1['s-1l'] = df1.index.to_series().apply(lambda x : get_previous_store_lower(x, dfs))\n",
    "\t\tdf1['s-1u'] = df1.index.to_series().apply(lambda x : get_previous_store_upper(x, dfs))\n",
    "\t\tdf1['s-2l'] = df1.index.to_series().apply(lambda x : get_previous_store_lower2(x, dfs))\n",
    "\t\tdf1['s-2u'] = df1.index.to_series().apply(lambda x : get_previous_store_upper2(x, dfs))\n",
    "\t\tstore_columns = pd.DataFrame(index=df1.index)\n",
    "\t\tstore_columns['s-1l'] = df1['s-1l']\n",
    "\t\tstore_columns['s-1u'] = df1['s-1u']\n",
    "\t\tstore_columns['s-2l'] = df1['s-2l']\n",
    "\t\tstore_columns['s-2u'] = df1['s-2u']\n",
    "\t\tstore_columns.to_csv(store_file)\n",
    "\n",
    "\t# Reorder the columns so the 'outputs' are the last 2 columns\n",
    "\tdf1 = df1[[\"pc-l\", \"ea-l\", \"ea-u\", \"v0l-1\", \"v0u-1\", \"v0l-2\", \"v0u-2\", \"v0l-3\", \\\n",
    "\t           \"v0u-3\", \"v0l-4\", \"v0u-4\", \"s-1l\", \"s-1u\", \"s-2l\", \"s-2u\", \"v0-l\", \"v0-u\"]]\n",
    "\n",
    "\t# Drop all rows with nans\n",
    "\treturn df1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../data/1M-LdSt.csv\"\n",
    "df = prepDataFrame(filename)\n",
    "num_training_examples = int(0.8*len(df))\n",
    "X_tr, X_te, y_tr, y_te = test_train_split(df, num_training_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---e = 100 w = 100 r = 0.00075 i = 0---\n",
      "rmse = 0.0012421019647526953\n",
      "---e = 100 w = 100 r = 0.00075 i = 1---\n",
      "rmse = 0.0012698088988954946\n",
      "---e = 100 w = 100 r = 0.00075 i = 2---\n",
      "rmse = 0.0012797240004229307\n",
      "---e = 100 w = 100 r = 0.00075 i = 3---\n",
      "rmse = 0.0012935658062535635\n",
      "---e = 100 w = 100 r = 0.00075 i = 4---\n",
      "rmse = 0.0012672786043439808\n",
      "INFO:tensorflow:Assets written to: 1M-LdSt-best-model/assets\n"
     ]
    }
   ],
   "source": [
    "widths = [100] #[100, 90, 80, 70, 60, 50]#[40,30,20,10,5]\n",
    "rates  = [0.00075] #[0.002, 0.001, 0.0005]#[0.001, 0.0001, 0.00001]\n",
    "epochs = [100] #[100]\n",
    "exp_data = pd.DataFrame(columns=['epochs', 'width', 'rate', 'error'])\n",
    "\n",
    "best_model = build_model()\n",
    "best_error = 10000\n",
    "\n",
    "row = 0\n",
    "for e in epochs:\n",
    "    for w in widths:\n",
    "        for r in rates:\n",
    "            # do 5 experiments for each hyperparameter combo\n",
    "            for i in range(5):\n",
    "                print(\"---e = \" + str(e) + \" w = \" + str(w) + ' r = ' + str(r) + ' i = ' + str(i) + \"---\")\n",
    "                # build, train, and test model. then compute the error\n",
    "                model = build_model(do1=w, do2=w, do3=w, do4=w, do5=w, l_rate=r)\n",
    "                model.fit(X_tr, y_tr, epochs=e, verbose=False)\n",
    "                predictions = model.predict(X_te)\n",
    "                rmse = get_rmse(predictions, y_te, y_te.shape[0])\n",
    "\n",
    "                # record the network paramaters and error\n",
    "                exp_data.loc[row] = [e, w, r, rmse]\n",
    "                row += 1\n",
    "\n",
    "                print(\"rmse = \" + str(rmse))\n",
    "                # see if this model is the best yet, if so save it\n",
    "                if rmse < best_error:\n",
    "                    best_error = rmse\n",
    "                    best_model = model\n",
    "\n",
    "# record the experimental data\n",
    "exp_data.to_csv(filename[:-4] + '-experiments.csv',index=False)\n",
    "# Save the best model\n",
    "best_model.save(filename[:-4] + '-best-model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load a previously saved model:\n",
    "# model = keras.models.load_model('path/to/location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break everyting up into 16 bits, if that doesnt work then add more stores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
